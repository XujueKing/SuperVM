# ğŸŒ æ™ºèƒ½åˆ†å¸ƒå¼å­˜å‚¨ä¸æ€§èƒ½ä¼˜åŒ–æ–¹æ¡ˆ

## æ ¸å¿ƒç†å¿µ: **æ•°æ®éšç½‘ç»œæ‹“æ‰‘è‡ªé€‚åº”åˆ†å¸ƒ**

å››å±‚ç½‘ç»œä¸­çš„å­˜å‚¨ä¸æ˜¯é™æ€çš„,è€Œæ˜¯æ ¹æ®ä»¥ä¸‹å› ç´ **åŠ¨æ€è°ƒæ•´**:

1. **èŠ‚ç‚¹è´Ÿè½½** (CPU/å†…å­˜/ç£ç›˜ IO)
2. **ç½‘ç»œæ‹“æ‰‘** (èŠ‚ç‚¹åœ°ç†ä½ç½®/å»¶è¿Ÿ/å¸¦å®½)
3. **æ•°æ®çƒ­åº¦** (è®¿é—®é¢‘ç‡/æœ€è¿‘è®¿é—®æ—¶é—´)
4. **å®¹é‡çŠ¶æ€** (ç£ç›˜ä½¿ç”¨ç‡/å‰©ä½™ç©ºé—´)
5. **èŠ‚ç‚¹å¥åº·** (æ•…éšœç‡/å“åº”æ—¶é—´/åœ¨çº¿æ—¶é•¿)

---

## ğŸ—ï¸ åˆ†å¸ƒå¼å­˜å‚¨æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    æ™ºèƒ½è°ƒåº¦å±‚                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  StorageOrchestrator (å­˜å‚¨ç¼–æ’å™¨)                 â”‚   â”‚
â”‚  â”‚  - ç›‘æ§æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€                                 â”‚   â”‚
â”‚  â”‚  - å†³ç­–æ•°æ®è¿ç§»/å¤åˆ¶                                â”‚   â”‚
â”‚  â”‚  - æ‰§è¡Œè‡ªåŠ¨è´Ÿè½½å‡è¡¡                                 â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                â†“                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   L1 å­˜å‚¨    â”‚  â”‚   L2 å­˜å‚¨    â”‚  â”‚   L3 å­˜å‚¨    â”‚
â”‚  RocksDB     â”‚  â”‚  RocksDB     â”‚  â”‚  LRU Cache   â”‚
â”‚  å…¨é‡+æƒå¨   â”‚  â”‚  éƒ¨åˆ†+æ´»è·ƒ   â”‚  â”‚  çƒ­ç‚¹+ä¸´æ—¶   â”‚
â”‚  10-100 TB   â”‚  â”‚  500GB-2TB   â”‚  â”‚  100GB-1TB   â”‚
â”‚  3å‰¯æœ¬+BFT   â”‚  â”‚  2å‰¯æœ¬+RAFT  â”‚  â”‚  æ— å‰¯æœ¬      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†“                â†“                â†“
    [çƒ­æ•°æ®è‡ªåŠ¨ä¸Šæµ®]  [æ¸©æ•°æ®æ™ºèƒ½ç¼“å­˜]  [å†·æ•°æ®è‡ªåŠ¨ä¸‹æ²‰]
```

### å››å±‚å­˜å‚¨åˆ†å·¥

| å±‚çº§ | å­˜å‚¨ç±»å‹ | å®¹é‡èŒƒå›´ | æ•°æ®å†…å®¹ | å‰¯æœ¬ç­–ç•¥ | è®¿é—®æ¨¡å¼ |
|-----|---------|---------|---------|---------|---------|
| **L1 Supernode** | RocksDB (NVMe) | 10-100 TB | å…¨é‡å†å²+æƒå¨çŠ¶æ€ | 3å‰¯æœ¬ + BFT | ä½é¢‘å…¨å±€æŸ¥è¯¢ |
| **L2 Miner** | RocksDB (SSD) | 500GB-2TB | æœ€è¿‘Nå—+æ´»è·ƒè´¦æˆ· | 2å‰¯æœ¬ + RAFT | ä¸­é¢‘äº¤æ˜“æ‰§è¡Œ |
| **L3 Edge** | LRU Cache (å†…å­˜) | 100GB-1TB | çƒ­ç‚¹æ•°æ®+åŒºåŸŸç¼“å­˜ | æ— å‰¯æœ¬ | é«˜é¢‘ç”¨æˆ·æŸ¥è¯¢ |
| **L4 Mobile** | SQLite (æœ¬åœ°) | 1-10GB | ç”¨æˆ·ä¸“å±+ç¦»çº¿é˜Ÿåˆ— | æ— å‰¯æœ¬ | æœ¬åœ°æ“ä½œ |

---

## ğŸ“Š å­˜å‚¨åˆ†çº§ç­–ç•¥ (ä¸‰æ¸©å­˜å‚¨)

### è®¾è®¡ç†å¿µ

åŒºå—é“¾çŠ¶æ€æ•°æ®å¹¶é"ä¸€è§†åŒä»",è€Œæ˜¯åˆ†ä¸º:
- **çƒ­æ•°æ®** (1%): é«˜é¢‘è®¿é—® (å¦‚çƒ­é—¨ NFT, DeFi åˆçº¦) â†’ NVMe + å¤§å†…å­˜
- **æ¸©æ•°æ®** (19%): ä¸­é¢‘è®¿é—® (å¦‚æ´»è·ƒè´¦æˆ·) â†’ SATA SSD + é€‚ä¸­ç¼“å­˜
- **å†·æ•°æ®** (80%): ç½•è§è®¿é—® (å¦‚å†å²åŒºå—) â†’ HDD + é«˜å‹ç¼©

é€šè¿‡**è‡ªåŠ¨åˆ†çº§**,å¯èŠ‚çœ 70% å­˜å‚¨æˆæœ¬,åŒæ—¶ä¿æŒçƒ­æ•°æ®æè‡´æ€§èƒ½ã€‚

### ä»£ç å®ç°

```rust
// src/node-core/src/storage/tiered_storage.rs

pub struct TieredStorage {
    hot_tier: HotStorage,     // çƒ­æ•°æ®: SSD/NVMe + å¤§å†…å­˜ç¼“å­˜
    warm_tier: WarmStorage,   // æ¸©æ•°æ®: SSD + é€‚ä¸­ç¼“å­˜
    cold_tier: ColdStorage,   // å†·æ•°æ®: HDD + å°ç¼“å­˜/æ— ç¼“å­˜
    classifier: DataClassifier, // æ•°æ®åˆ†ç±»å™¨
}

/// æ•°æ®æ¸©åº¦åˆ†ç±»å™¨
pub struct DataClassifier {
    access_tracker: DashMap<Key, AccessStats>,
    hot_threshold: u32,   // è®¿é—®æ¬¡æ•° > 1000/å¤©
    warm_threshold: u32,  // è®¿é—®æ¬¡æ•° 100-1000/å¤©
}

pub struct AccessStats {
    pub total_accesses: AtomicU64,
    pub last_access_time: AtomicU64,
    pub access_pattern: AccessPattern,  // éšæœº/é¡ºåº/æ‰¹é‡
}

#[derive(Debug, Clone)]
pub enum AccessPattern {
    Random,      // éšæœºè®¿é—® (è´¦æˆ·ä½™é¢æŸ¥è¯¢)
    Sequential,  // é¡ºåºè®¿é—® (åŒºå—æ‰«æ)
    Batch,       // æ‰¹é‡è®¿é—® (æ‰¹é‡è½¬è´¦)
    RareWrite,   // ç½•è§å†™å…¥ (å†å²æ•°æ®ä¿®æ­£)
}

impl DataClassifier {
    /// æ ¹æ®è®¿é—®æ¨¡å¼å†³å®šæ•°æ®å­˜å‚¨å±‚çº§
    pub fn classify(&self, key: &Key) -> StorageTier {
        let stats = self.access_tracker.get(key);
        
        match stats {
            Some(s) => {
                let accesses_per_day = s.total_accesses.load(Ordering::Relaxed) 
                    / self.days_since_creation();
                let hours_since_access = self.hours_since(s.last_access_time);
                
                // å†³ç­–æ ‘
                if accesses_per_day > self.hot_threshold && hours_since_access < 1 {
                    StorageTier::Hot
                } else if accesses_per_day > self.warm_threshold && hours_since_access < 24 {
                    StorageTier::Warm
                } else {
                    StorageTier::Cold
                }
            }
            None => StorageTier::Cold,  // æ–°æ•°æ®é»˜è®¤å†·å­˜å‚¨
        }
    }
    
    /// å‘¨æœŸæ€§é‡æ–°åˆ†ç±» (æ¯å°æ—¶æ‰§è¡Œ)
    pub async fn reclassify_all(&self) -> Result<ReclassifyReport> {
        let mut moved_to_hot = 0;
        let mut moved_to_warm = 0;
        let mut moved_to_cold = 0;
        
        for entry in self.access_tracker.iter() {
            let key = entry.key();
            let new_tier = self.classify(key);
            let current_tier = self.get_current_tier(key)?;
            
            if new_tier != current_tier {
                self.migrate_data(key, current_tier, new_tier).await?;
                
                match new_tier {
                    StorageTier::Hot => moved_to_hot += 1,
                    StorageTier::Warm => moved_to_warm += 1,
                    StorageTier::Cold => moved_to_cold += 1,
                }
            }
        }
        
        Ok(ReclassifyReport {
            moved_to_hot,
            moved_to_warm,
            moved_to_cold,
            total_keys: self.access_tracker.len(),
        })
    }
}
```

### ä¸‰æ¸©å­˜å‚¨é…ç½®

```rust
/// ä¸‰æ¸©å­˜å‚¨é…ç½®
pub struct StorageConfig {
    pub hot: HotStorageConfig,
    pub warm: WarmStorageConfig,
    pub cold: ColdStorageConfig,
}

pub struct HotStorageConfig {
    pub device: String,              // "/dev/nvme0n1" (NVMe SSD)
    pub cache_size_mb: usize,        // 16GB å†…å­˜ç¼“å­˜
    pub write_buffer_mb: usize,      // 512MB å†™ç¼“å†²
    pub bloom_filter_bits: usize,    // 10 bits/key
    pub compression: CompressionType, // LZ4 (å¿«é€Ÿå‹ç¼©)
    pub max_open_files: usize,       // 10000
    pub target_iops: usize,          // 500K read, 300K write
}

pub struct WarmStorageConfig {
    pub device: String,              // "/dev/sda1" (SATA SSD)
    pub cache_size_mb: usize,        // 4GB å†…å­˜ç¼“å­˜
    pub write_buffer_mb: usize,      // 128MB å†™ç¼“å†²
    pub bloom_filter_bits: usize,    // 8 bits/key
    pub compression: CompressionType, // Zstd (ä¸­ç­‰å‹ç¼©)
    pub max_open_files: usize,       // 5000
    pub target_iops: usize,          // 200K read, 100K write
}

pub struct ColdStorageConfig {
    pub device: String,              // "/dev/sdb1" (HDD)
    pub cache_size_mb: usize,        // 512MB å†…å­˜ç¼“å­˜
    pub write_buffer_mb: usize,      // 32MB å†™ç¼“å†²
    pub bloom_filter_bits: usize,    // 5 bits/key
    pub compression: CompressionType, // Zstd level 10 (é«˜å‹ç¼©)
    pub max_open_files: usize,       // 1000
    pub target_iops: usize,          // 10K read, 5K write
}
```

### æ€§èƒ½å¯¹æ¯”

| å­˜å‚¨å±‚çº§ | è®¾å¤‡ç±»å‹ | ç¼“å­˜å¤§å° | è¯»IOPS | å†™IOPS | å»¶è¿Ÿ | æˆæœ¬/TB |
|---------|---------|---------|--------|--------|------|---------|
| **Hot** | NVMe SSD | 16GB | 500K | 300K | 0.1ms | $300 |
| **Warm** | SATA SSD | 4GB | 200K | 100K | 1ms | $100 |
| **Cold** | HDD | 512MB | 10K | 5K | 10ms | $20 |

**æˆæœ¬ä¼˜åŒ–ç¤ºä¾‹**:
- ä¼ ç»Ÿæ–¹æ¡ˆ: 100TB Ã— $300 = **$30,000**
- ä¸‰æ¸©åˆ†çº§: 1TB(Hot) Ã— $300 + 19TB(Warm) Ã— $100 + 80TB(Cold) Ã— $20 = **$3,500** (èŠ‚çœ 88%)

---

## ğŸ”„ è‡ªåŠ¨æ•°æ®è¿ç§»æœºåˆ¶

### è¿ç§»è§¦å‘æ¡ä»¶

```rust
// src/node-core/src/storage/data_migration.rs

pub struct DataMigrationManager {
    orchestrator: Arc<StorageOrchestrator>,
    migration_queue: Arc<Mutex<VecDeque<MigrationTask>>>,
    worker_threads: usize,
}

pub struct MigrationTask {
    pub key_range: (Key, Key),       // æ•°æ®èŒƒå›´
    pub from_node: NodeId,            // æºèŠ‚ç‚¹
    pub to_node: NodeId,              // ç›®æ ‡èŠ‚ç‚¹
    pub priority: MigrationPriority,  // ä¼˜å…ˆçº§
    pub reason: MigrationReason,      // è¿ç§»åŸå› 
}

#[derive(Debug, Clone, Copy, PartialEq, Eq, PartialOrd, Ord)]
pub enum MigrationPriority {
    Critical = 3,  // èŠ‚ç‚¹å³å°†ä¸‹çº¿/ç£ç›˜å°†æ»¡
    High = 2,      // è´Ÿè½½ä¸¥é‡ä¸å‡
    Normal = 1,    // ä¼˜åŒ–æ€§èƒ½
    Low = 0,       // åå°æ•´ç†
}

#[derive(Debug, Clone)]
pub enum MigrationReason {
    NodeOverload { cpu: f64, disk_io: f64 },  // èŠ‚ç‚¹è¿‡è½½
    DiskFull { usage_percent: f64 },          // ç£ç›˜å°†æ»¡
    HotDataReplication,                       // çƒ­æ•°æ®å¤åˆ¶åˆ°æ›´å¤šèŠ‚ç‚¹
    ColdDataArchive,                          // å†·æ•°æ®å½’æ¡£åˆ°HDD
    NetworkOptimization,                      // å°†æ•°æ®ç§»åˆ°ç¦»ç”¨æˆ·æ›´è¿‘çš„èŠ‚ç‚¹
    NodeFailure { failed_node: NodeId },      // èŠ‚ç‚¹æ•…éšœæ¢å¤
}
```

### è‡ªåŠ¨æ£€æµ‹ä¸æ‰§è¡Œ

```rust
impl DataMigrationManager {
    /// è‡ªåŠ¨æ£€æµ‹éœ€è¦è¿ç§»çš„æ•°æ®
    pub async fn detect_migration_needs(&self) -> Result<Vec<MigrationTask>> {
        let mut tasks = Vec::new();
        let nodes = self.orchestrator.get_all_nodes().await?;
        
        for node in nodes {
            // 1. æ£€æµ‹ç£ç›˜ä½¿ç”¨ç‡
            if node.disk_usage_percent > 85.0 {
                let task = self.create_disk_relief_task(node).await?;
                tasks.push(task);
            }
            
            // 2. æ£€æµ‹è´Ÿè½½ä¸å‡
            if node.cpu_usage > 80.0 || node.disk_io_usage > 90.0 {
                let task = self.create_load_balance_task(node).await?;
                tasks.push(task);
            }
            
            // 3. æ£€æµ‹çƒ­æ•°æ®éœ€è¦å¤åˆ¶
            let hot_keys = self.orchestrator.get_hot_keys(node.id).await?;
            if hot_keys.len() > 100 {
                let task = self.create_hot_replication_task(node, hot_keys).await?;
                tasks.push(task);
            }
            
            // 4. æ£€æµ‹å†·æ•°æ®å¯ä»¥å½’æ¡£
            let cold_keys = self.orchestrator.get_cold_keys(node.id).await?;
            if cold_keys.len() > 10000 {
                let task = self.create_cold_archive_task(node, cold_keys).await?;
                tasks.push(task);
            }
        }
        
        // 5. æŒ‰ä¼˜å…ˆçº§æ’åº
        tasks.sort_by(|a, b| b.priority.cmp(&a.priority));
        
        Ok(tasks)
    }
    
    /// æ‰§è¡Œæ•°æ®è¿ç§»
    pub async fn execute_migration(&self, task: MigrationTask) -> Result<MigrationResult> {
        let start_time = Instant::now();
        
        // 1. é¢„æ£€æŸ¥
        let source_available = self.orchestrator.check_node_health(&task.from_node).await?;
        let target_available = self.orchestrator.check_node_health(&task.to_node).await?;
        
        if !source_available || !target_available {
            return Err(anyhow!("Source or target node unavailable"));
        }
        
        // 2. æ‰¹é‡è¯»å–æºæ•°æ® (é¿å…å•æ¡è¯»å–)
        let data = self.batch_read_range(
            &task.from_node,
            &task.key_range.0,
            &task.key_range.1,
        ).await?;
        
        let total_keys = data.len();
        let total_bytes = data.iter().map(|(_, v)| v.len()).sum::<usize>();
        
        // 3. æ‰¹é‡å†™å…¥ç›®æ ‡èŠ‚ç‚¹
        self.batch_write(&task.to_node, data.clone()).await?;
        
        // 4. éªŒè¯æ•°æ®å®Œæ•´æ€§
        let verification_samples = self.sample_keys(&data, 100);
        for (key, expected_value) in verification_samples {
            let actual_value = self.read_single(&task.to_node, &key).await?;
            if actual_value != expected_value {
                return Err(anyhow!("Data verification failed for key {:?}", key));
            }
        }
        
        // 5. åˆ é™¤æºæ•°æ® (å¦‚æœæ˜¯ç§»åŠ¨æ“ä½œ)
        if matches!(task.reason, MigrationReason::DiskFull { .. } | MigrationReason::ColdDataArchive) {
            self.batch_delete(&task.from_node, data.keys().cloned().collect()).await?;
        }
        
        Ok(MigrationResult {
            total_keys,
            total_bytes,
            duration: start_time.elapsed(),
            throughput_mbps: (total_bytes as f64 / 1_000_000.0) / start_time.elapsed().as_secs_f64(),
        })
    }
    
    /// æ™ºèƒ½è´Ÿè½½å‡è¡¡ (è‡ªåŠ¨åœ¨ L1/L2 èŠ‚ç‚¹é—´å¹³è¡¡æ•°æ®)
    pub async fn auto_rebalance(&self) -> Result<RebalanceReport> {
        let nodes = self.orchestrator.get_all_nodes().await?;
        
        // 1. è®¡ç®—å¹³å‡è´Ÿè½½
        let avg_disk_usage: f64 = nodes.iter()
            .map(|n| n.disk_usage_percent)
            .sum::<f64>() / nodes.len() as f64;
        
        let avg_cpu_usage: f64 = nodes.iter()
            .map(|n| n.cpu_usage)
            .sum::<f64>() / nodes.len() as f64;
        
        // 2. è¯†åˆ«è¿‡è½½å’Œç©ºé—²èŠ‚ç‚¹
        let overloaded: Vec<_> = nodes.iter()
            .filter(|n| n.disk_usage_percent > avg_disk_usage + 20.0 
                     || n.cpu_usage > avg_cpu_usage + 20.0)
            .collect();
        
        let underloaded: Vec<_> = nodes.iter()
            .filter(|n| n.disk_usage_percent < avg_disk_usage - 20.0 
                     && n.cpu_usage < avg_cpu_usage - 20.0)
            .collect();
        
        if overloaded.is_empty() || underloaded.is_empty() {
            return Ok(RebalanceReport::no_action_needed());
        }
        
        // 3. åˆ›å»ºè¿ç§»ä»»åŠ¡
        let mut tasks = Vec::new();
        for (over, under) in overloaded.iter().zip(underloaded.iter()) {
            let keys_to_move = self.select_keys_to_move(over, under).await?;
            
            tasks.push(MigrationTask {
                key_range: keys_to_move,
                from_node: over.id,
                to_node: under.id,
                priority: MigrationPriority::Normal,
                reason: MigrationReason::NodeOverload {
                    cpu: over.cpu_usage,
                    disk_io: over.disk_io_usage,
                },
            });
        }
        
        // 4. æ‰§è¡Œè¿ç§»
        let mut results = Vec::new();
        for task in tasks {
            let result = self.execute_migration(task).await?;
            results.push(result);
        }
        
        Ok(RebalanceReport {
            tasks_executed: results.len(),
            total_keys_moved: results.iter().map(|r| r.total_keys).sum(),
            total_bytes_moved: results.iter().map(|r| r.total_bytes).sum(),
            total_duration: results.iter().map(|r| r.duration).sum(),
        })
    }
}
```

### è¿ç§»åœºæ™¯ç¤ºä¾‹

| åœºæ™¯ | è§¦å‘æ¡ä»¶ | è¿ç§»ç­–ç•¥ | é¢„æœŸæ•ˆæœ |
|-----|---------|---------|---------|
| **ç£ç›˜å°†æ»¡** | ä½¿ç”¨ç‡ > 85% | å†·æ•°æ® â†’ HDD å½’æ¡£ | é‡Šæ”¾ 50-70% ç©ºé—´ |
| **èŠ‚ç‚¹è¿‡è½½** | CPU > 80% æˆ– IO > 90% | å‡è¡¡æ•°æ®åˆ°ç©ºé—²èŠ‚ç‚¹ | è´Ÿè½½é™ä½ 40-60% |
| **çƒ­æ•°æ®æ¶Œç°** | è®¿é—®é¢‘ç‡æš´å¢ | å¤åˆ¶åˆ°å¤šä¸ª L2/L3 | å»¶è¿Ÿé™ä½ 80% |
| **èŠ‚ç‚¹æ•…éšœ** | å¿ƒè·³ä¸¢å¤± > 30s | å‰¯æœ¬è¿ç§»åˆ°å¥åº·èŠ‚ç‚¹ | æ¢å¤æ—¶é—´ < 5min |
| **åœ°ç†ä¼˜åŒ–** | è·¨åŒºåŸŸå»¶è¿Ÿ > 100ms | æ•°æ®è¿ç§»åˆ°æœ¬åœ° L2 | å»¶è¿Ÿé™ä½ 70% |

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–æå‡æ–¹æ¡ˆ

### ç›®æ ‡æ€§èƒ½ (vs Phase 4.2 åŸè®¡åˆ’)

| æ“ä½œç±»å‹ | åŸè®¡åˆ’æ€§èƒ½ | ä¼˜åŒ–ç›®æ ‡ | æå‡å€æ•° | ä¼˜åŒ–ç­–ç•¥ |
|---------|-----------|---------|---------|---------|
| éšæœºè¯» | 100K ops/s | **500K ops/s** | 5Ã— | Bloom filter + åˆ†å±‚ç¼“å­˜ + å¹¶è¡Œè¯» |
| éšæœºå†™ | 50K ops/s | **300K ops/s** | 6Ã— | Write batching + WALä¼˜åŒ– + å¼‚æ­¥åˆ·ç›˜ |
| æ‰¹é‡å†™ | 200K ops/s | **1M ops/s** | 5Ã— | å¤§æ‰¹é‡ + Pipeline + å‹ç¼©å¹¶è¡Œ |
| æ‰«æ | 500 MB/s | **2 GB/s** | 4Ã— | Prefetch + é¡ºåºè¯»ä¼˜åŒ– + é›¶æ‹·è´ |
| ç‚¹æŸ¥å»¶è¿Ÿ P99 | 10 ms | **2 ms** | 5Ã— | çƒ­æ•°æ®å†…å­˜åŒ– + NVMe + ç´¢å¼•ä¼˜åŒ– |

### ä¼˜åŒ–ç­–ç•¥è¯¦è§£

```rust
// src/node-core/src/storage/optimized_rocksdb.rs

pub struct OptimizedRocksDB {
    db: Arc<DB>,
    hot_cache: Arc<DashMap<Key, Value>>,      // çƒ­æ•°æ®å…¨å†…å­˜
    write_buffer: Arc<Mutex<WriteBatch>>,     // å†™ç¼“å†²
    bloom_filter: Arc<BloomFilter>,           // Bloom è¿‡æ»¤å™¨
    prefetch_engine: Arc<PrefetchEngine>,     // é¢„å–å¼•æ“
    parallel_reader: Arc<ParallelReader>,     // å¹¶è¡Œè¯»å–å™¨
}

impl OptimizedRocksDB {
    /// åˆ›å»ºé«˜æ€§èƒ½é…ç½®
    pub fn new_high_performance(path: &str, tier: StorageTier) -> Result<Self> {
        let mut opts = Options::default();
        opts.create_if_missing(true);
        
        // æ ¹æ®å­˜å‚¨å±‚çº§å®šåˆ¶é…ç½®
        match tier {
            StorageTier::Hot => {
                // çƒ­æ•°æ®: æè‡´æ€§èƒ½
                opts.set_write_buffer_size(512 * 1024 * 1024);  // 512MB
                opts.set_max_write_buffer_number(6);
                opts.set_min_write_buffer_number_to_merge(2);
                opts.set_level_zero_file_num_compaction_trigger(4);
                opts.set_max_background_jobs(16);               // 16 ä¸ªåå°çº¿ç¨‹
                opts.set_max_subcompactions(4);
                opts.set_compression_type(DBCompressionType::Lz4);  // å¿«é€Ÿå‹ç¼©
                opts.set_bloom_locality(1);
                opts.set_memtable_prefix_bloom_ratio(0.1);
                opts.set_allow_mmap_reads(true);                // å†…å­˜æ˜ å°„è¯»
                opts.set_allow_mmap_writes(true);               // å†…å­˜æ˜ å°„å†™
                
                // å—ç¼“å­˜ 16GB
                let cache = Cache::new_lru_cache(16 * 1024 * 1024 * 1024);
                let mut block_opts = BlockBasedOptions::default();
                block_opts.set_block_cache(&cache);
                block_opts.set_block_size(64 * 1024);           // 64KB å—
                block_opts.set_cache_index_and_filter_blocks(true);
                block_opts.set_pin_l0_filter_and_index_blocks_in_cache(true);
                block_opts.set_bloom_filter(10.0, false);       // 10 bits/key
                opts.set_block_based_table_factory(&block_opts);
            }
            
            StorageTier::Warm => {
                // æ¸©æ•°æ®: å¹³è¡¡æ€§èƒ½
                opts.set_write_buffer_size(128 * 1024 * 1024);  // 128MB
                opts.set_max_write_buffer_number(4);
                opts.set_max_background_jobs(8);
                opts.set_compression_type(DBCompressionType::Zstd);
                
                // å—ç¼“å­˜ 4GB
                let cache = Cache::new_lru_cache(4 * 1024 * 1024 * 1024);
                let mut block_opts = BlockBasedOptions::default();
                block_opts.set_block_cache(&cache);
                block_opts.set_bloom_filter(8.0, false);
                opts.set_block_based_table_factory(&block_opts);
            }
            
            StorageTier::Cold => {
                // å†·æ•°æ®: èŠ‚çœç©ºé—´
                opts.set_write_buffer_size(32 * 1024 * 1024);   // 32MB
                opts.set_max_write_buffer_number(2);
                opts.set_max_background_jobs(4);
                opts.set_compression_type(DBCompressionType::Zstd);
                opts.set_compression_options(10, 0, 0, 0);      // æœ€é«˜å‹ç¼©
                
                // å—ç¼“å­˜ 512MB
                let cache = Cache::new_lru_cache(512 * 1024 * 1024);
                let mut block_opts = BlockBasedOptions::default();
                block_opts.set_block_cache(&cache);
                block_opts.set_bloom_filter(5.0, false);
                opts.set_block_based_table_factory(&block_opts);
            }
        }
        
        let db = DB::open(&opts, path)?;
        
        Ok(Self {
            db: Arc::new(db),
            hot_cache: Arc::new(DashMap::new()),
            write_buffer: Arc::new(Mutex::new(WriteBatch::default())),
            bloom_filter: Arc::new(BloomFilter::new(1_000_000, 0.01)),
            prefetch_engine: Arc::new(PrefetchEngine::new()),
            parallel_reader: Arc::new(ParallelReader::new(16)),  // 16 å¹¶è¡Œè¯»
        })
    }
    
    /// é«˜æ€§èƒ½éšæœºè¯» (ç›®æ ‡ 500K ops/s)
    pub async fn get_optimized(&self, key: &Key) -> Result<Option<Value>> {
        // Level 1: çƒ­ç¼“å­˜ (å†…å­˜, <100ns)
        if let Some(value) = self.hot_cache.get(key) {
            return Ok(Some(value.clone()));
        }
        
        // Level 2: Bloom è¿‡æ»¤å™¨å¿«é€Ÿæ’é™¤ä¸å­˜åœ¨çš„ key (<1Î¼s)
        if !self.bloom_filter.contains(key) {
            return Ok(None);
        }
        
        // Level 3: RocksDB å—ç¼“å­˜ (~10Î¼s)
        let value = self.db.get(key)?;
        
        // æ›´æ–°çƒ­ç¼“å­˜
        if let Some(ref v) = value {
            self.hot_cache.insert(key.clone(), v.clone());
        }
        
        Ok(value)
    }
    
    /// æ‰¹é‡å¹¶è¡Œè¯» (ç›®æ ‡ 500K ops/s)
    pub async fn multi_get_parallel(&self, keys: Vec<Key>) -> Result<Vec<Option<Value>>> {
        // 1. æ‹†åˆ†ä¸ºçƒ­ç¼“å­˜å‘½ä¸­å’Œæœªå‘½ä¸­
        let mut cached = Vec::new();
        let mut uncached_keys = Vec::new();
        let mut uncached_indices = Vec::new();
        
        for (i, key) in keys.iter().enumerate() {
            if let Some(value) = self.hot_cache.get(key) {
                cached.push((i, Some(value.clone())));
            } else {
                uncached_keys.push(key.clone());
                uncached_indices.push(i);
            }
        }
        
        // 2. å¹¶è¡Œè¯»å–æœªç¼“å­˜çš„ keys (16 çº¿ç¨‹å¹¶è¡Œ)
        let uncached_values = self.parallel_reader
            .read_batch(&self.db, uncached_keys)
            .await?;
        
        // 3. åˆå¹¶ç»“æœ
        let mut result = vec![None; keys.len()];
        for (i, value) in cached {
            result[i] = value;
        }
        for (i, value) in uncached_indices.into_iter().zip(uncached_values.into_iter()) {
            result[i] = value;
        }
        
        Ok(result)
    }
    
    /// é«˜æ€§èƒ½éšæœºå†™ (ç›®æ ‡ 300K ops/s)
    pub async fn put_optimized(&self, key: Key, value: Value) -> Result<()> {
        // 1. ç«‹å³æ›´æ–°çƒ­ç¼“å­˜ (ä¿è¯è¯»ä¸€è‡´æ€§)
        self.hot_cache.insert(key.clone(), value.clone());
        
        // 2. æ·»åŠ åˆ°å†™ç¼“å†² (å¼‚æ­¥åˆ·ç›˜)
        {
            let mut buffer = self.write_buffer.lock().await;
            buffer.put(&key, &value);
            
            // 3. è¾¾åˆ°é˜ˆå€¼æ—¶æ‰¹é‡åˆ·ç›˜ (1000 æ¡æˆ– 1MB)
            if buffer.len() >= 1000 || buffer.size_in_bytes() >= 1_000_000 {
                let batch = std::mem::replace(&mut *buffer, WriteBatch::default());
                drop(buffer);  // é‡Šæ”¾é”
                
                // å¼‚æ­¥åˆ·ç›˜ (ä¸é˜»å¡åç»­å†™å…¥)
                let db = self.db.clone();
                tokio::spawn(async move {
                    if let Err(e) = db.write(batch) {
                        eprintln!("Failed to flush write batch: {}", e);
                    }
                });
            }
        }
        
        // 4. æ›´æ–° Bloom è¿‡æ»¤å™¨
        self.bloom_filter.insert(&key);
        
        Ok(())
    }
    
    /// è¶…é«˜æ€§èƒ½æ‰¹é‡å†™ (ç›®æ ‡ 1M ops/s)
    pub async fn batch_write_optimized(&self, entries: Vec<(Key, Value)>) -> Result<()> {
        let start = Instant::now();
        
        // 1. æ‰¹é‡æ›´æ–°çƒ­ç¼“å­˜ (å¹¶è¡Œ)
        entries.par_iter().for_each(|(k, v)| {
            self.hot_cache.insert(k.clone(), v.clone());
        });
        
        // 2. æ„å»ºå¤§æ‰¹é‡å†™å…¥
        let mut batch = WriteBatch::default();
        for (key, value) in entries.iter() {
            batch.put(key, value);
            self.bloom_filter.insert(key);
        }
        
        // 3. å¯ç”¨ Pipeline æ¨¡å¼æ‰¹é‡å†™å…¥
        let mut write_opts = WriteOptions::default();
        write_opts.set_sync(false);        // å¼‚æ­¥åˆ·ç›˜
        write_opts.disable_wal(false);     // ä¿ç•™ WAL (ä¿è¯æŒä¹…æ€§)
        
        self.db.write_opt(batch, &write_opts)?;
        
        let duration = start.elapsed();
        let throughput = entries.len() as f64 / duration.as_secs_f64();
        
        if throughput < 1_000_000.0 {
            eprintln!("Warning: Batch write throughput {} ops/s < target 1M ops/s", 
                      throughput as usize);
        }
        
        Ok(())
    }
    
    /// æ™ºèƒ½é¢„å– (å‡å°‘éšæœºè¯»å»¶è¿Ÿ)
    pub async fn prefetch_related_keys(&self, key: &Key) -> Result<()> {
        // 1. é¢„æµ‹å¯èƒ½è®¿é—®çš„ç›¸å…³ keys (åŸºäºè®¿é—®æ¨¡å¼)
        let related_keys = self.prefetch_engine.predict_next_keys(key).await?;
        
        // 2. åå°æ‰¹é‡é¢„å–åˆ°ç¼“å­˜
        let db = self.db.clone();
        let hot_cache = self.hot_cache.clone();
        
        tokio::spawn(async move {
            for key in related_keys {
                if let Ok(Some(value)) = db.get(&key) {
                    hot_cache.insert(key, value);
                }
            }
        });
        
        Ok(())
    }
}
```

### å¹¶è¡Œè¯»å–å™¨

```rust
/// å¹¶è¡Œè¯»å–å™¨
pub struct ParallelReader {
    thread_pool: ThreadPool,
}

impl ParallelReader {
    pub fn new(num_threads: usize) -> Self {
        Self {
            thread_pool: rayon::ThreadPoolBuilder::new()
                .num_threads(num_threads)
                .build()
                .unwrap(),
        }
    }
    
    /// å¹¶è¡Œè¯»å–å¤šä¸ª keys
    pub async fn read_batch(
        &self,
        db: &DB,
        keys: Vec<Key>,
    ) -> Result<Vec<Option<Value>>> {
        let (tx, rx) = tokio::sync::mpsc::channel(keys.len());
        
        // å°† keys åˆ†é…åˆ°çº¿ç¨‹æ± å¹¶è¡Œè¯»å–
        let db = Arc::new(db.clone());
        self.thread_pool.scope(|s| {
            for (i, key) in keys.into_iter().enumerate() {
                let db = db.clone();
                let tx = tx.clone();
                s.spawn(move |_| {
                    let value = db.get(&key).ok().flatten();
                    let _ = tx.blocking_send((i, value));
                });
            }
        });
        
        drop(tx);  // å…³é—­å‘é€ç«¯
        
        // æ”¶é›†ç»“æœ
        let mut results: Vec<(usize, Option<Value>)> = Vec::new();
        let mut rx = rx;
        while let Some((i, value)) = rx.recv().await {
            results.push((i, value));
        }
        
        // æŒ‰åŸå§‹é¡ºåºæ’åº
        results.sort_by_key(|(i, _)| *i);
        Ok(results.into_iter().map(|(_, v)| v).collect())
    }
}
```

### é¢„å–å¼•æ“ (æœºå™¨å­¦ä¹ )

```rust
/// é¢„å–å¼•æ“ (æœºå™¨å­¦ä¹ é¢„æµ‹ä¸‹ä¸€æ¬¡è®¿é—®)
pub struct PrefetchEngine {
    access_history: Arc<Mutex<VecDeque<Key>>>,
    pattern_model: Arc<Mutex<HashMap<Key, Vec<Key>>>>,  // key â†’ åç»­è®¿é—®çš„ keys
}

impl PrefetchEngine {
    pub fn new() -> Self {
        Self {
            access_history: Arc::new(Mutex::new(VecDeque::with_capacity(1000))),
            pattern_model: Arc::new(Mutex::new(HashMap::new())),
        }
    }
    
    /// é¢„æµ‹ä¸‹ä¸€æ¬¡å¯èƒ½è®¿é—®çš„ keys
    pub async fn predict_next_keys(&self, current_key: &Key) -> Result<Vec<Key>> {
        let model = self.pattern_model.lock().await;
        
        Ok(model.get(current_key)
            .cloned()
            .unwrap_or_default())
    }
    
    /// è®°å½•è®¿é—®å¹¶æ›´æ–°æ¨¡å‹
    pub async fn record_access(&self, key: Key) -> Result<()> {
        let mut history = self.access_history.lock().await;
        
        // è®°å½•å½“å‰è®¿é—®
        if history.len() >= 1000 {
            history.pop_front();
        }
        history.push_back(key.clone());
        
        // æ›´æ–°è®¿é—®æ¨¡å¼ (å½“å‰ key åç»å¸¸è®¿é—®çš„ keys)
        if history.len() >= 2 {
            let prev_key = history[history.len() - 2].clone();
            let mut model = self.pattern_model.lock().await;
            
            model.entry(prev_key)
                .or_insert_with(Vec::new)
                .push(key);
        }
        
        Ok(())
    }
}
```

---

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”æ€»ç»“

### å®Œæ•´æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | Phase 4.2 åŸè®¡åˆ’ | ä¼˜åŒ–åç›®æ ‡ | æå‡å€æ•° | å…³é”®æŠ€æœ¯ |
|-----|----------------|----------|---------|---------|
| éšæœºè¯» | 100K ops/s | **500K ops/s** | 5Ã— | çƒ­ç¼“å­˜ + Bloom + å¹¶è¡Œè¯» |
| éšæœºå†™ | 50K ops/s | **300K ops/s** | 6Ã— | Write batching + å¼‚æ­¥åˆ·ç›˜ |
| æ‰¹é‡å†™ | 200K ops/s | **1M ops/s** | 5Ã— | å¤§æ‰¹é‡ + Pipeline |
| æ‰«æ | 500 MB/s | **2 GB/s** | 4Ã— | Prefetch + é›¶æ‹·è´ |
| å»¶è¿Ÿ P99 | 10 ms | **2 ms** | 5Ã— | å…¨å†…å­˜çƒ­æ•°æ® |
| å­˜å‚¨æˆæœ¬ | åŸºå‡† | **-70%** | - | ä¸‰æ¸©åˆ†çº§ + é«˜å‹ç¼© |
| è´Ÿè½½å‡è¡¡ | æ‰‹åŠ¨ | **è‡ªåŠ¨** | - | æ™ºèƒ½è¿ç§» + ç›‘æ§ |

### å®é™…åº”ç”¨åœºæ™¯æ€§èƒ½

| åœºæ™¯ | æ“ä½œç±»å‹ | åŸæ€§èƒ½ | ä¼˜åŒ–å | ç”¨æˆ·ä½“éªŒæå‡ |
|-----|---------|-------|-------|------------|
| **NFT æŸ¥è¯¢** | éšæœºè¯» | 100K/s | 500K/s | é¡µé¢åŠ è½½ 10ms â†’ 2ms |
| **æ‰¹é‡è½¬è´¦** | æ‰¹é‡å†™ | 200K/s | 1M/s | 1000 ç¬” 5s â†’ 1s |
| **DeFi äº¤æ˜“** | éšæœºè¯»å†™ | æ··åˆ | 3-6Ã— | äº¤æ˜“ç¡®è®¤ 50ms â†’ 10ms |
| **åŒºå—æ‰«æ** | é¡ºåºè¯» | 500MB/s | 2GB/s | 100GB æ‰«æ 200s â†’ 50s |
| **å†å²å½’æ¡£** | å†·æ•°æ® | - | -70% æˆæœ¬ | é•¿æœŸå­˜å‚¨å¯æ‰¿å— |

---

## ğŸ¯ å®æ–½ä¼˜å…ˆçº§ä¸æ—¶é—´è¡¨

### ä¼˜åŒ–æŠ€æœ¯ä¼˜å…ˆçº§çŸ©é˜µ

| ä¼˜åŒ–æŠ€æœ¯ | æ€§èƒ½æå‡ | å®ç°å¤æ‚åº¦ | ä¼˜å…ˆçº§ | é¢„è®¡å‘¨æœŸ | ä¾èµ–é¡¹ |
|---------|---------|-----------|--------|---------|--------|
| çƒ­æ•°æ®å…¨å†…å­˜ç¼“å­˜ | è¯» 5Ã—, å»¶è¿Ÿ 10Ã— | ä½ | ğŸ”´ é«˜ | 3å¤© | æ—  |
| Write Batching + å¼‚æ­¥åˆ·ç›˜ | å†™ 6Ã— | ä¸­ | ğŸ”´ é«˜ | 5å¤© | æ—  |
| Bloom Filter | è¯» 2Ã— (é¿å…æ— æ•ˆæŸ¥è¯¢) | ä½ | ğŸ”´ é«˜ | 2å¤© | æ—  |
| å¹¶è¡Œè¯»å– (16çº¿ç¨‹) | æ‰¹é‡è¯» 4Ã— | ä¸­ | ğŸŸ¡ ä¸­ | 1å‘¨ | Rayon |
| æ™ºèƒ½é¢„å– | å»¶è¿Ÿ -50% | é«˜ | ğŸŸ¡ ä¸­ | 2å‘¨ | è®¿é—®ç»Ÿè®¡ |
| ä¸‰æ¸©å­˜å‚¨åˆ†çº§ | æˆæœ¬ -70% | é«˜ | ğŸŸ¢ ä½ | 2å‘¨ | æ•°æ®åˆ†ç±»å™¨ |
| è‡ªåŠ¨æ•°æ®è¿ç§» | è´Ÿè½½å‡è¡¡ +80% | é«˜ | ğŸŸ¢ ä½ | 3å‘¨ | å­˜å‚¨ç¼–æ’å™¨ |

### åˆ†é˜¶æ®µå®æ–½è®¡åˆ’

#### **Week 1-2: å¿«é€Ÿè§æ•ˆé˜¶æ®µ**

**ç›®æ ‡**: 3-4Ã— æ€§èƒ½æå‡

**ä»»åŠ¡**:
- âœ… å®ç° DashMap çƒ­æ•°æ®ç¼“å­˜
- âœ… å®ç° WriteBatch ç´¯ç§¯åˆ·ç›˜
- âœ… é›†æˆ Bloom Filter
- âœ… RocksDB é…ç½®ä¼˜åŒ– (Hot tier)

**é¢„æœŸç»“æœ**:
- è¯»æ€§èƒ½: 100K â†’ 300K ops/s
- å†™æ€§èƒ½: 50K â†’ 200K ops/s
- å»¶è¿Ÿ: 10ms â†’ 4ms

**éªŒæ”¶æ ‡å‡†**:
```bash
cargo bench --bench optimized_rocksdb_bench
# éšæœºè¯» >= 300K ops/s
# éšæœºå†™ >= 200K ops/s
# P99 å»¶è¿Ÿ <= 5ms
```

#### **Week 3-4: æ€§èƒ½ä¼˜åŒ–é˜¶æ®µ**

**ç›®æ ‡**: 5-6Ã— æ€§èƒ½æå‡

**ä»»åŠ¡**:
- âœ… å®ç° ParallelReader (16 çº¿ç¨‹)
- âœ… é…ç½®ä¸‰æ¸©å­˜å‚¨ (Hot/Warm/Cold)
- âœ… å®ç° DataClassifier (è®¿é—®ç»Ÿè®¡)
- âœ… é›†æˆ Zstd å‹ç¼©

**é¢„æœŸç»“æœ**:
- è¯»æ€§èƒ½: 300K â†’ 500K ops/s
- å†™æ€§èƒ½: 200K â†’ 300K ops/s
- æ‰¹é‡å†™: 200K â†’ 800K ops/s
- æˆæœ¬: åŸºå‡† â†’ -50%

**éªŒæ”¶æ ‡å‡†**:
```bash
cargo bench --bench parallel_read_bench
# æ‰¹é‡è¯» (1000 keys) <= 2ms
# æ‰¹é‡å†™ (10K entries) <= 10ms
# å†·æ•°æ®å‹ç¼©æ¯” >= 3:1
```

#### **Week 5-6: é•¿æœŸä¼˜åŒ–é˜¶æ®µ**

**ç›®æ ‡**: æ™ºèƒ½åŒ– + è‡ªåŠ¨åŒ–

**ä»»åŠ¡**:
- âœ… å®ç° PrefetchEngine (æœºå™¨å­¦ä¹ )
- âœ… å®ç° DataMigrationManager (è‡ªåŠ¨è¿ç§»)
- âœ… å®ç° StorageOrchestrator (å…¨å±€è°ƒåº¦)
- âœ… é›†æˆ Prometheus + Grafana ç›‘æ§

**é¢„æœŸç»“æœ**:
- å»¶è¿Ÿ: 4ms â†’ 2ms (é¢„å–ç”Ÿæ•ˆ)
- è´Ÿè½½å‡è¡¡: æ‰‹åŠ¨ â†’ è‡ªåŠ¨ (ä¸å‡è¡¡åº¦ < 20%)
- æˆæœ¬: -50% â†’ -70% (ä¸‰æ¸©ç”Ÿæ•ˆ)
- è¿ç»´: è¢«åŠ¨ â†’ ä¸»åŠ¨ (è‡ªåŠ¨è¿ç§»)

**éªŒæ”¶æ ‡å‡†**:
```bash
# 1. é¢„å–å‘½ä¸­ç‡æµ‹è¯•
cargo test --test prefetch_accuracy_test
# å‘½ä¸­ç‡ >= 60%

# 2. è‡ªåŠ¨è¿ç§»æµ‹è¯•
cargo test --test auto_migration_test
# ç£ç›˜ä½¿ç”¨ç‡ä¸å‡è¡¡åº¦ <= 20%

# 3. ç«¯åˆ°ç«¯æ€§èƒ½æµ‹è¯•
cargo bench --bench e2e_storage_bench
# éšæœºè¯» >= 500K ops/s
# æ‰¹é‡å†™ >= 1M ops/s
# P99 å»¶è¿Ÿ <= 2ms
```

---

## ğŸ” ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### Prometheus æŒ‡æ ‡

```rust
// src/node-core/src/storage/metrics.rs

pub struct StorageMetrics {
    // è¯»å†™æ€§èƒ½
    pub read_ops_total: Counter,
    pub write_ops_total: Counter,
    pub read_latency: Histogram,
    pub write_latency: Histogram,
    
    // ç¼“å­˜å‘½ä¸­ç‡
    pub cache_hits_total: Counter,
    pub cache_misses_total: Counter,
    pub bloom_filter_hits: Counter,
    pub bloom_filter_misses: Counter,
    
    // å­˜å‚¨åˆ†çº§
    pub hot_tier_size_bytes: Gauge,
    pub warm_tier_size_bytes: Gauge,
    pub cold_tier_size_bytes: Gauge,
    pub tier_migrations_total: Counter,
    
    // æ•°æ®è¿ç§»
    pub migrations_in_progress: Gauge,
    pub migrations_success_total: Counter,
    pub migrations_failure_total: Counter,
    pub migration_throughput_mbps: Gauge,
    
    // è´Ÿè½½å‡è¡¡
    pub node_disk_usage_percent: GaugeVec,  // label: node_id
    pub node_cpu_usage_percent: GaugeVec,   // label: node_id
    pub load_imbalance_score: Gauge,
}
```

### Grafana ä»ªè¡¨ç›˜

**æ ¸å¿ƒé¢æ¿**:
1. **æ€§èƒ½æ€»è§ˆ**: è¯»å†™ OPS, å»¶è¿Ÿ P50/P99, ååé‡
2. **ç¼“å­˜æ•ˆç‡**: çƒ­ç¼“å­˜å‘½ä¸­ç‡, Bloom è¿‡æ»¤ç‡, é¢„å–å‘½ä¸­ç‡
3. **å­˜å‚¨åˆ†çº§**: Hot/Warm/Cold å æ¯”, è¿ç§»è¶‹åŠ¿, æˆæœ¬èŠ‚çœ
4. **è´Ÿè½½å‡è¡¡**: èŠ‚ç‚¹è´Ÿè½½åˆ†å¸ƒ, ä¸å‡è¡¡åº¦, è¿ç§»ä»»åŠ¡é˜Ÿåˆ—
5. **å‘Šè­¦**: ç£ç›˜å°†æ»¡, èŠ‚ç‚¹è¿‡è½½, è¿ç§»å¤±è´¥, æ€§èƒ½ä¸‹é™

---

## ğŸ“š å‚è€ƒæ–‡æ¡£

- [å››å±‚ç½‘ç»œç¡¬ä»¶éƒ¨ç½²ä¸ç®—åŠ›è°ƒåº¦](./å››å±‚ç½‘ç»œç¡¬ä»¶éƒ¨ç½²ä¸ç®—åŠ›è°ƒåº¦.md)
- [SuperVM ä¸æ•°æ®åº“çš„å…³ç³»](./SuperVMä¸æ•°æ®åº“çš„å…³ç³».md)
- [RocksDB æ€§èƒ½è°ƒä¼˜æŒ‡å—](https://github.com/facebook/rocksdb/wiki/RocksDB-Tuning-Guide)
- [Phase 4.2 æŒä¹…åŒ–å­˜å‚¨é›†æˆ](../../ROADMAP.md#phase-42-æŒä¹…åŒ–å­˜å‚¨é›†æˆ)
