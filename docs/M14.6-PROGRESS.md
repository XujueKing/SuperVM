# Phase 14 M14.6: Persistent Backend - Device-Local 路径评估

Status: In Progress (Day 1)
Date: 2025-11-13

## Update · A/B 合并、大块流式与批处理流水化（2025-11-13）

### 已完成优化

1. **A/B 合并为单一输入缓冲（A||B）**
   - 每个 chunk 一次 `cmd_copy_buffer` 带两段区域，减少拷贝与屏障调用次数
   - Descriptor bindings 使用不同 offset 访问同一缓冲的 A/B 区域

2. **可调 chunk 粒度**
   - 环境变量 `GPU_STREAM_CHUNK_ELEMS`（默认 524,288）
   - 可设置 1,048,576 进行更大块评估

3. **轻量批处理接口**
   - `run_field_add_batch(...)`：顺序执行 2–4 个作业，复用缓存 pipeline

4. **批处理流水化（Ping-Pong 双缓冲）** ⭐新增⭐
   - `run_field_add_batch_pipelined(...)`：使用两套 device/staging 缓冲集
   - Transfer 队列预拷贝作业 N+1 的同时，compute 队列执行作业 N
   - 通过信号量跨队列同步（copy_in_done → compute → compute_done → copy_out）
   - **实测加速**：4 作业 @ 16K 元素 → **1.79x**（53ms → 29ms）

### 基准结果（`bench_batch_pipelined`）

| 规模 | 批次大小 | 顺序执行 | 流水化执行 | 加速比 |
|------|---------|----------|-----------|--------|
| 16K  | 2 jobs  | 27 ms    | 23 ms     | 1.13x  |
| 16K  | 3 jobs  | 41 ms    | 25 ms     | 1.64x  |
| 16K  | 4 jobs  | 53 ms    | 29 ms     | **1.79x** |
| 64K  | 4 jobs  | 104 ms   | 70 ms     | 1.48x  |
| 256K | 4 jobs  | 262 ms   | 217 ms    | 1.20x  |

### 使用方式（PowerShell）

```powershell

# 设置设备本地路径和大块大小（单次作业流式）

$env:GPU_DEVICE_LOCAL='1'
$env:GPU_STREAM_CHUNK_ELEMS='1048576'
cargo run -p gpu-executor --features spirv-vulkan --example bench_persistent

# 批处理流水化基准（自动使用 device-local + ping-pong）

cargo run -p gpu-executor --features spirv-vulkan --example bench_batch_pipelined --release

```

## 目标

在 Persistent Backend 上实现并评估 Device-Local 缓冲区路径（使用 staging buffer 进行数据上传/下载），对比 Host-Visible 路径的延迟与吞吐表现，寻找规模阈值与优化方向。

## 实现内容

- 在 `PersistentVulkanBackend` 增加 `execute_device_local_raw()`：
  - A/B/C 使用 `DEVICE_LOCAL` 存储：
    - A、B: `STORAGE_BUFFER | TRANSFER_DST`
    - C: `STORAGE_BUFFER | TRANSFER_SRC`
  - Staging 缓冲：
    - A、B: `TRANSFER_SRC | HOST_VISIBLE | HOST_COHERENT`
    - C: `TRANSFER_DST | HOST_VISIBLE | HOST_COHERENT`
  - 同一命令缓冲中完成：Host→Device Copy → Barrier(TRANSFER→COMPUTE) → Dispatch → Barrier(COMPUTE→TRANSFER) → Device→Host Copy

- 基准扩展 `examples/bench_persistent.rs`：
  - 输出两组持久化数据：Host-Visible 与 Device-Local

## 基准结果（本机 64→64K 元素）

- One-Shot (baseline): 94–111ms（固定开销主导）

- Persistent Host-Visible：Avg 4.65ms → 16.87ms（64→64K）

- Persistent Device-Local：Avg 8.80ms → 26.76ms（64→64K）

结论：在≤64K 规模下，Persistent + Host-Visible 显著优于 Device-Local（后者增加了两次拷贝与屏障成本）。

## 分析与推断

- Persistent 架构下已消除了 100ms 级别初始化成本，数据传输成为主要开销；对于中小规模，Host-Visible 的直接映射成本更低。

- Device-Local 路径仅在“极大规模”（预计 ≥1–4M 元素）时可能胜出，此时带宽与 GPU 计算更占比，额外拷贝可被摊薄。

## 下一步计划（建议）

1. ✅ ~~批量/异步优化~~ **已完成**
   - ✅ A、B 合并为单一缓冲
   - ✅ 引入信号量序列化，实现 Transfer 与 Compute 重叠（ping-pong 双缓冲）
2. ✅ ~~阈值策略~~ **已实现**
   - 基于元素规模自动选择 Host-Visible vs Device-Local（AUTO_DEVICE_LOCAL_THRESHOLD = 1M）
3. 内存池化（待实施）：
   - 复用 VkBuffer/VkDeviceMemory，避免频繁创建/销毁（当前每次作业仍创建新缓冲）
   - 可为批处理场景提供额外 10-20% 性能提升
4. 更大规模实验（待评估）：
   - 扩展基准至 1M、4M，重新评估 device-local 交叉点
   - 评估流水化在超大批次（8-16 jobs）下的扩展性
5. Workgroup 专化（可选）：
   - 依据数据规模与硬件特性动态选择 local_size_x（例如 64/128/256）
   - 通过 specialization constants 编译期优化

## 影响与决策

- 短期内将 Persistent Backend 的默认路径设为 Host-Visible；

- 保留 Device-Local 开关用于大规模或特定硬件测试；

- 后续通过内存池与异步流水化减少 Device-Local 的“额外拷贝”成本。

--
Owner: Phase 14 Team
Reviewer: Pending
