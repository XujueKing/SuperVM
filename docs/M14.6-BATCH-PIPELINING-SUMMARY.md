# Phase 14 M14.6: 批处理流水化总结

**完成日期**: 2025-11-13  
**状态**: ✅ 完成

## 实现内容

### 1. A/B 输入合并优化

- 将独立的 A、B 缓冲合并为单一 `(A||B)` 输入缓冲

- 每个 chunk 一次 `cmd_copy_buffer` 调用，两段区域（A 段 + B 段）

- Descriptor bindings 使用不同 offset 访问同一缓冲

- **收益**: 减少 50% 的拷贝命令与屏障开销

### 2. 可调 Chunk 粒度

- 新增环境变量 `GPU_STREAM_CHUNK_ELEMS`

- 默认值: 524,288 (512K 元素)

- 可选值: 1,048,576 (1M 元素) 用于大规模评估

- **用途**: 针对不同硬件特性调优拷贝/计算平衡点

### 3. 批处理接口（顺序版）

```rust
pub fn run_field_add_batch(
    &mut self,
    spirv_bytes: &[u8],
    jobs: &[(&[u64], &[u64])],
    elements: usize,
    device_local: bool,
) -> Result<Vec<Vec<u64>>, VulkanError>

```

- 顺序执行多个作业，复用缓存 pipeline

- 适用于简单批量场景（无跨作业重叠）

### 4. 批处理流水化（Ping-Pong 双缓冲）⭐

```rust
pub fn run_field_add_batch_pipelined(
    &mut self,
    spirv_bytes: &[u8],
    jobs: &[(&[u64], &[u64])],
    elements: usize,
) -> Result<Vec<Vec<u64>>, VulkanError>

```

**架构设计**:

- **双缓冲集**: 两套独立的 device-local + staging 缓冲（ping/pong）

- **交替执行**: 作业 N 使用 buffer_set_0，作业 N+1 使用 buffer_set_1

- **跨队列协调**:
  ```
  Job N:   copy_in[N] → compute[N] → copy_out[N]
             |            |            |
             transfer_q   compute_q    transfer_q
             ↓            ↓            ↓
           sem[N]       sem[N]       sem[N]
  ```

- **重叠效果**: Transfer 队列为作业 N+1 拷贝输入时，Compute 队列并行执行作业 N

## 性能基准

### Release 模式基准 (`bench_batch_pipelined`)

| 规模 | 批次 | 顺序执行 | 流水化 | 加速比 | 每作业时间 (流水化) |
|------|------|----------|--------|--------|-------------------|
| 16K  | 2    | 27 ms    | 23 ms  | 1.13x  | 11.5 ms/job       |
| 16K  | 3    | 41 ms    | 25 ms  | 1.64x  | 8.3 ms/job        |
| 16K  | 4    | 53 ms    | 29 ms  | **1.79x** | **7.3 ms/job** |
| 64K  | 2    | 52 ms    | 56 ms  | 0.93x  | 28.0 ms/job       |
| 64K  | 3    | 82 ms    | 59 ms  | 1.40x  | 19.7 ms/job       |
| 64K  | 4    | 104 ms   | 70 ms  | **1.48x** | **17.5 ms/job** |
| 256K | 2    | 171 ms   | 186 ms | 0.92x  | 93.0 ms/job       |
| 256K | 3    | 229 ms   | 215 ms | 1.07x  | 71.7 ms/job       |
| 256K | 4    | 262 ms   | 217 ms | 1.20x  | 54.3 ms/job       |

### 关键发现

1. **批次规模效应**: 加速比随批次增大而提升（2→4 jobs: 1.1x → 1.8x）
2. **规模依赖性**: 中小规模（16K-64K）受益最大，超大规模（256K）收益减少（计算占比更高）
3. **最佳点**: **4 jobs @ 16K 元素** 达到 **1.79x** 加速，每作业耗时从 13.3ms → 7.3ms
4. **Overhead 权衡**: 2 jobs 时流水化开销可能抵消收益，需 ≥3 jobs 才稳定获益

## 技术栈

### 核心技术

- **Vulkan 1.2**: async compute + transfer queues

- **Binary Semaphores**: 跨队列同步信号

- **Pipeline Barriers**: memory dependency 管理

- **Command Buffer Reuse**: 从 command pool 分配/释放

### 资源管理

```rust
// 每个 buffer set 包含:

- buf_in_dev:  DEVICE_LOCAL | TRANSFER_DST  (A||B combined)

- buf_out_dev: DEVICE_LOCAL | TRANSFER_SRC  (C)

- buf_in_stg:  HOST_VISIBLE | TRANSFER_SRC  (staging A||B)

- buf_out_stg: HOST_VISIBLE | TRANSFER_DST  (staging C)

```

### 同步机制

```rust
// 3 个信号量/作业:
copy_in_done[job_idx]   // transfer → compute
compute_done[job_idx]   // compute → transfer
copy_out_done[job_idx]  // transfer → CPU (via fence)

```

## 使用示例

### 批处理流水化（推荐）

```bash
cargo run -p gpu-executor --features spirv-vulkan \
  --example bench_batch_pipelined --release

```

### 单作业流式（大块调参）

```powershell
$env:GPU_DEVICE_LOCAL='1'
$env:GPU_STREAM_CHUNK_ELEMS='1048576'
cargo run -p gpu-executor --features spirv-vulkan \
  --example bench_persistent

```

## 代码文件

| 文件 | 说明 |
|------|------|
| `src/spirv/persistent_backend.rs` | 核心实现（1588 行，新增 ~220 行） |
| `examples/bench_batch_pipelined.rs` | 流水化基准测试 |
| `examples/bench_persistent.rs` | 单作业基准（已扩展至 1M） |
| `docs/M14.6-PROGRESS.md` | 详细进度文档 |

## 后续优化方向

### 短期（Phase 14 后续）

1. **内存池化**: 复用 VkBuffer/VkDeviceMemory（减少 alloc/free 开销）
2. **Timeline Semaphores**: 替代多个 binary semaphores，简化同步逻辑
3. **Profiler 集成**: Vulkan 时间戳查询，精确测量各阶段耗时

### 中期（Phase 15+）

1. **更大批次**: 测试 8-16 jobs 扩展性（需评估 descriptor pool 容量）
2. **自适应调度**: 基于硬件特性动态选择 chunk size 与 batch size
3. **Multi-GPU**: 跨设备批处理（不同作业分发到不同 GPU）

### 长期（架构级）

1. **Persistent mapped buffers**: 避免每次 map/unmap 开销
2. **Compute-Transfer 融合**: 在支持的硬件上合并队列（减少同步）
3. **SPIR-V specialization**: workgroup size / chunk size 编译期常量化

## 结论

M14.6 成功实现了**批处理流水化架构**，通过 ping-pong 双缓冲实现了跨作业的拷贝/计算重叠，在 4 作业 @ 16K 元素场景下达到 **1.79x 加速**。该架构为后续高吞吐 GPU 计算场景（如 MSM batch proving、FFT 流水线）奠定了基础，展示了 Persistent Backend 在资源复用与并行调度上的优势。

---

**贡献者**: Phase 14 Team  
**审核**: Pending  
**文档版本**: v1.0 (2025-11-13)
